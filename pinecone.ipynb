{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 31\n",
      "Number of document chunks: 265\n",
      "base_url='http://localhost:11434' model='llama2' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please set PINECONE_API_KEY and PINECONE_API_ENV environment variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m environment \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPINECONE_API_ENV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m environment:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set PINECONE_API_KEY and PINECONE_API_ENV environment variables.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m pinecone\u001b[38;5;241m.\u001b[39minit(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Define index name\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Please set PINECONE_API_KEY and PINECONE_API_ENV environment variables."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Step 1: Set up environment variables\n",
    "PINECONE_API_KEY = 'your-pinecone-api-key'\n",
    "\n",
    "# Step 2: Initialize Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Step 3: Create or connect to a Pinecone index\n",
    "index_name = \"pdf-store-open-source\"\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Sentence Transformer 'all-MiniLM-L6-v2' embedding dimension\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
    "    )\n",
    "\n",
    "# Step 4: Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = \"path/to/your/document.pdf\"\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Step 5: Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)\n",
    "\n",
    "# Step 6: Create Document objects\n",
    "documents = [Document(page_content=t) for t in texts]\n",
    "\n",
    "# Step 7: Initialize Sentence Transformer embeddings\n",
    "embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 8: Create embeddings and store in Pinecone\n",
    "index = pc.Index(index_name)\n",
    "for i, doc in enumerate(documents):\n",
    "    embedding = embeddings_model.encode(doc.page_content).tolist()\n",
    "    index.upsert(vectors=[(str(i), embedding, {\"text\": doc.page_content})])\n",
    "\n",
    "print(f\"Successfully stored {len(documents)} document chunks in Pinecone index '{index_name}'\")\n",
    "\n",
    "# Step 9: Load FLAN-T5 model locally\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Step 10: Perform a similarity search and generate response\n",
    "def query_and_respond(query, k=3):\n",
    "    query_embedding = embeddings_model.encode(query).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=k, include_metadata=True)\n",
    "    \n",
    "    context = \" \".join([match['metadata']['text'] for match in results['matches']])\n",
    "    \n",
    "    prompt = f\"Answer the following question based on this context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=150, num_return_sequences=1, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the main topic of this document?\"\n",
    "answer = query_and_respond(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
